{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First, L2 regularization for *logistic regression* model.\n",
    "\n",
    "The multinomial logistic regression model with stochastic gradient descent is adapted from assignment 2:2_fullyconnected.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    '''\n",
    "    Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch.  \n",
    "    '''\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)  # Learning rate for regularization\n",
    "    \n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + \n",
    "                          beta * tf.nn.l2_loss(weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 48.787804\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 13.3%\n",
      "Minibatch loss at step 500: 0.727448\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 1000: 0.799105\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 1500: 0.566019\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 2000: 0.648290\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2500: 0.782455\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 3000: 0.784387\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.4%\n",
      "Test accuracy: 88.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps+1):\n",
    "        '''\n",
    "        Pick an offset within the training data, which has been randomized.\n",
    "        Note: we could use better randomization across epochs.\n",
    "        '''\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        '''\n",
    "        Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        and the value is the numpy array to feed to it.\n",
    "        '''\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta: 0.01}  # Use 0.01 for beta\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from assignment 2 that the final result without regularization is:\n",
    "\n",
    "###### Minibatch loss at step 3000: *1.061334*\n",
    "###### Minibatch accuracy: *77.3%*\n",
    "###### Validation accuracy: *79.0%*\n",
    "###### Test accuracy: *86.4%*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try different values for beta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% finished\n",
      "16.0% finished\n",
      "33.0% finished\n",
      "50.0% finished\n",
      "66.0% finished\n",
      "83.0% finished\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "beta_values = [pow(10, i) for i in np.arange(-4, -1, 0.1)] # from 0.0001 to 0.1\n",
    "results = np.zeros(len(beta_values))\n",
    "\n",
    "for i, beta_value in enumerate(beta_values):\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        \n",
    "        tf.initialize_all_variables().run()        \n",
    "        for step in range(num_steps+1):\n",
    "            \n",
    "            '''\n",
    "            Pick an offset within the training data, which has been randomized.\n",
    "            Note: we could use better randomization across epoch\n",
    "            '''\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            '''\n",
    "            Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            and the value is the numpy array to feed to it.\n",
    "            '''\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : beta_value}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            \n",
    "        results[i] = accuracy(test_prediction.eval(), test_labels)\n",
    "        # Print the progress\n",
    "        if(i%5 == 0):\n",
    "            print('%.1f%% finished' %(100*i/len(beta_values)))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEdCAYAAADNU1r0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XecVOX1x/HPF1ARVFbFAqhgDXZsiH0VFWIvWEARLFix\nxhYbsSUaf5pYggZFRaWoxK5RRFkjloCCgAr2BaQYiRSNhXZ+fzx3cVxnd2dnZ+bOnT3v12tfzJ3b\nzs5l59z7nHufR2aGc845l40mcQfgnHMuuTyJOOecy5onEeecc1nzJOKccy5rnkScc85lzZOIc865\nrHkScYklabmkTbJcd0NJiyQpxzHtKWlqLrfpXDHzJOIaRFKlpO+jL+TZkh6Q1KJAu8/6ISczm2lm\na1gDH5SqnsjMbKyZbdmQbdaxv5aSvpP0fL724Vx9eBJxDWXAwWa2BtAJ2AH4fYH2ndVVhKSmOYyh\n0E/rHg38CBwgad1C7jjHn5srEZ5EXC4IwMz+A7xESCZhhrSypP+TNF3SHEkDJa2SMv/S6ArmS0mn\npp7ZSxoj6ZSUZftIej1tANJBkiZIWhjta0DKvPbRdk+RNB14JeW9JpK6SPo2uppaJOkHSZ9H6+4i\n6U1J8yXNknSnpGbRvNei331ytN4xkvaRNDNl3x2j32O+pCmSDk2Z94CkuyQ9F63/lqSN6/is+wB3\nA5OBE6t9BhtI+oek/0j6WtIdKfP6Sfow2s/7kjpF7//iSiqK6bro9T6SZkbHaA5wv6QySc9G+/hv\n9LptyvprSro/+qz+K+mJ6P0pkg5OWa5ZFOP2dfy+rsh5EnE5I2kD4LfAJylv3wxsBmwX/dsOuCZa\nvjtwAbBfNK+cus/sa5r/HdDbzFoBBwNnSjqs2jJ7Ax2BbqnbMrO3zWz16GpqLeDfwLBomWVRjGsB\nu0Wxnh2tt0+0zLZR09jjqduNks2zwIvAOsB5wFBJm6fEdBwwACgDPgNurOkXl9Se8BkNjeLrkzKv\nCfAc8AWwEeFzHhHNO4bwmZ8Y/Y6HAf9NjbUW60exbQScTvjOuB/YMHrve+BvKcs/AqwKbAmsC/wl\nev8hoHfKcgcDs81sUh37d8XOzPzHf7L+IXxpLYp+lgMvA2ukzP8O2Dhlejfg8+j1YODGlHmbRtvY\nJJoeA5ySMr8P8K+U6RXLponrL8Ct0ev2hGTQPmV+1XtNqq13N/BMLb/v+cA/aooB2AeYEb3ei/BF\nmbr+MOCa6PUDwKCUeb8FPqxl31cBE6LXbYElwPbRdBfgq+q/TzTvReDcGrZZPf4HgOtSfpcfgZVq\niakT8N/odRtgaerxT1muDbAQWC2afhy4OO7/v/7T8B+/EnG5cLiFM9x9CGf6rQEkrQO0AN6V9I2k\nb4B/AmtH67UFZqZsJ/V1vUjaVdKrUTPLAuCMqjhSfFnHNs4gXK30Snlv86jJZk603RvTbLcmbfj1\n7zSdcJVQZW7K6++B1WrZXm/CVQhmNhv4Fz9fjWwITDez5WnW25BwlZONr81sSdWEpFUl/T26oWIB\n8BpQJknABsA3Zrao+kbMbA7wBnC0pFaEhDk0y5hcEfEk4nKhqibyOjAEuDV6fx7hi3FrM1sr+imz\n0OQEMIfwxVNlo2rb/R8hCVVZv5YYhgJPAe3MrAz4e1VcKWpsupG0F3AtcJiZfZcy625gKrBptN0r\n02y3JrMJX+CpNgJmZbh+any7AZsDv48S2hygM9ArasqaCWwUva5uJuEqL53vqf0zrv6Z/S6KY5fo\n89i7KsRoP2tJWqOGfVU1aR0DvBklFpdwnkRcrv2VcOfQtmZmwL3AX6OrEiS1k3RgtOxjwMlR8bkF\nobkm9UvrPeCo6Ox3M+DUWva7GjDfzJZI6kzK1UQk3Re/opg2BB4FTjKz6mfsqwOLzOx7SR2Bs6rN\nnwvU9KzKv4Hvo8J0M0nlwCHA8Fp+j5r0BUYRag3bRz/bEhLAb4FxhKR8k6QWklaRtHu07n3AxZJ2\njH7fTaPfGWAiUSKKalRVdZ6arA78ACyStBbwh6oZZjaXcKU5MCrAN4uSc5WngB0JtaGHsvgMXBHy\nJOIa6hdnqmY2j3A1ck301uXAp8DbUfPHKGCLaNkXgTsItY+PgbeidX6K/v0Lod1/LqGt/pFa9n02\ncL2khYRk9GhtcVZ7bz9CEXhkdPfSt5KmRPMuBk6QtIhwdTOi2jb+ADwUNdf1qPZZLAEOBQ4iXJXd\nRSj+V914kNHtwQp3s/UA7jCzr83sP9FPJeHLuE/UjHUo4SphBuGq4NgojpGEZrhh0e/xJOFGAQg3\nDRwGzAd6RvNq81dC4poHvAm8UG1+b0JdZBqhRnN+yufxI/APYGPgiUx+d1f8FE4W87gD6ULCGeRy\nYApwMuFs6m6gJVAJnFCtCaFq3UpCMW45sMTMOuc1WBer6Ex/CrBKDW37LuEkXQ1sbmYnxR2Ly428\nXolE94+fC+xoZtsBzQhnO/cCl5rZ9oQzn0tr2MRyoNzMdvAEUpokHaHwLMmahNuBn/EEUpqi5q9T\nCVd0rkQUojmrKdAyumd+VUJRcXMzGxvNH014Cjcd4U1upe4M4D+EZ0uWED2D4UqLpNMIzWzPm9kb\nccfjcqcQzVnnEdpjvwdGmVlvSWOBP5vZM5IuAgak3LGTuu7nwALC/fyDzOzevAbrnHOuXprlc+OS\nyoDDCQ92LSQULnsBpwB3Ru2jzwCLa9jEHmY2J7qz52VJU1OuYFL3U+j+i5xzLvHMrMG9WOe7qWh/\nwtPJ35jZMsIdGbub2cdm1s3MdiHc7ZL2Qaiq+8jN7GtC7aTGukjcT20OGDCgKLZXn/XqWjbb+fV5\nP9efWzEcv2I4dnUtk828Yj1+pfi3l+vjl+69XMl3EpkBdJHUPHqitSswNeWZgSaE2zHvqb5idK/7\natHrlsCBwPt5jjdr5eXlRbG9+qxX17LZzq/P+5WVlbXuo1ByefyK4djVtUw284r1+JXi315dy9R3\nXq4/o1SFqIkMAI4nFE0nAqcRHtg6h3Cf/BNmdkW0bBvgXjM7RKE30yejZZoBQ83sphr2Yfn+PVx+\n9O3blwcffDDuMFyW/PgllyQsB81ZeU8iheBJJLkqKiryepbk8suPX3J5EknhScQ55+onV0nEn8Fw\nsaqoqIg7BNcAfvycJxHnnHNZ8+Ys55xrhLw5yznnXOw8ibhYeZt6svnxc55EnHPOZc1rIs451wh5\nTcQ551zsPIm4WHmberL58XOeRJxzzmXNayLOOdcIeU3EOedc7DyJuFgVQ5v6Z5/B0qVxR5FMxXD8\nXLw8ibhGa/Zs6NULtt8edtwRXn897oicSx5PIi5WcYxFsWQJ3HorbLcddOgAc+fClVdCz55w0klh\n2mXGxxJxnkRcIsycCfffD3PmNGw7FRXQqROMGgVvvAF//COsthocdxxMnQrrrw/bbgu3396wJq4f\nfoAnnoAxY8Dv+XClzJOIi1VdbeoLF8Lll4cv/qefhq22gv33h8GDYf78zPdT1XTVpw9cfz28+CL8\n5je/XGb11eHPf4Z//QueeQZ22gnGjs18H0uXwksvhX20bQsDB8KZZ8Lee8Mrr5RmMvGaiPMk4orS\n4sVwxx2wxRbw9dcweXJIIrNnw1lnwQsvhKaoI46Axx6D779Pv53qTVcffghHHQWq5cbGLbeE0aN/\nbuLq0we++ir9smbw5pvQvz+0awcDBoTkM3Vq2MaHH4ZEcvbZpZ1MXCNmZnn9AS4E3gcmA0OBlYHt\ngTeBScDTwGo1rNsdmAZ8DFxWyz7MlYbly80ef9xss83Munc3mzy55mUXLDB74AGzAw80KyszO/FE\nsxdeMFu8OMwfM8Zsq63C/GnTsotn0SKzSy4xa93a7PbbzZYsCTFOmmR2+eVm7duHfdxwg9mnn9a8\nnaVLzR55xGyLLcz23NNs9OiwHefiEn1vNvg7Pq8PG0pqC4wFOprZYkmPAi8A5wAXmdlYSX2BTczs\nmmrrNomSR1dgNjAeON7MpqXZj+Xz93CF8cYbcMkloZ5wyy2h2SpTX30VrkiGD4dPPw11jU8/hb/8\nBY48svYrj0xMnRquNv7zn3Al8e234SqlV6+wr0y3v2wZjBgB110H664Lf/gD7Ldfw+Nzrr5y9bBh\nvq9C2gLTgTWBZsAzwP7A/JRlNgA+SLNuF+CfKdOXU8PVCH4lklhjxoyxjz4yO/JIsw03NBsyxGzZ\nsoZt8/PPzR5+2Oy773ITY5Xly82ef95s7NiGx1gqVyZjxoyJOwSXJXJ0JdKswVmo9gQ1W9KtwAzg\ne2CUmY2W9IGkw8zsGeDYKJFU1w6YmTL9JdA5n/G6wlq0KNwF9frr4Qpk6FBYddWGb3fjjcNPrklw\n0EG52VbTpnDCCXD88eHK5OyzQ/1mo42gTZtQmK/+b9u2ofjvXDHJaxKRVAYcDrQHFgIjJfUCTgHu\nlHQ14epkcUP31bdvXzp06ABAWVkZnTp1WnEPe9UdJD5dXNOPPlpOkyblDB5cQatWsOqqxRVfIaab\nNoV27SoYOBDatStnzhwYPbqCb76B2bPLeecdmDq1gnnzYMGCciQoK6tg003hwQfL2XjjeOMvLy8v\nqs/Tp2uernpdWVlJLuW7JtID6GZm/aLp3sCuZtY/ZZnNgYfNrEu1dbsAfzCz7tH05YTLr5vT7Mfy\n+Xu43JsyBbp2hWnTYK214o4mGapqMbNnw1NPwf/9H1xzDZxzTriyca4+ktIB4wygi6TmkkQokk+V\ntA6sKJ5fBdyTZt3xwGaS2ktaGTiecNXiEs4MLrwwfAFOnlwRdziJIcEaa0DHjuHZmTffhJEjYa+9\nQuE/Dqlnua5xymsSMbNxwEhgIuF2XgGDgJ6SPgI+BGaZ2YMAktpIei5adxnQHxgFfACMMLOY/lRc\nLj37bDibPuOMuCNJti22CE/gn3hiSCQ33hjqKs4Vko8n4gpq8WLYemu46y7o1i3uaErH9OnhocbZ\ns0P3MDvtFHdErtglpTnLuV+4665wBu0JJLfatw9P8V98cbiD7PLLw/M2zuWbJxFXMF9/DX/6U+iG\npIq3qeeOBL17hy5iPvssdHGf7+7t/fg5TyKuYK65Jjwb0bFj3JGUtvXWg8cfh5tuCs+h9O8PP/4Y\nd1SuVHlNxBWE39Ibj/nz4fTTYcYM+Mc/YIN0j/W6RslrIi4xUm/p9QRSWGuuGfoUO/JI6NzZR290\nuedJxOVdbbf0ept6/kmh0H7//dCjB/ztb7nrjt6Pn/Mk4vJq8WL43e9Cb7orrRR3NI1b9+7hAcV7\n7oFTT/U6icsNr4m4vLrttjAQ0/PPxx2Jq/Ldd3DKKVBZGYbw9TpJ45SrmognEZc3X38dhrN9/XW/\nI6vYmIWhgG+/HR59NDzx7hoXL6y7opfJLb3eph4PCS67DB54oGF1Ej9+Lq9dwbvGa8qUcEvptF+N\nQ+mKSbduoU5yxBHw7rswcCA0bx53VC5JvDnL5ZwZHHBA+GLq37/u5V38UuskQ4fC5pvHHZHLN2/O\nckXLe+lNntVWC7WRE0+E3XYLY9wvXRp3VC4JPIm4nKrvLb3epl48JDjvPBg3Dl56KSSTyZNrX8eP\nn/Mk4nLmf/8LY4V7L73Jtskm8PLLoWv5rl3DDRI//RR3VK5YeU3E5cTo0aGPpj32CLeNevcmpWH2\n7HBi8MknMHgwdOlS9zouGfw5kRSeROKzYEFovnr55fAk9EEHxR2RyzWz0Cvw+eeHXoFvuAFatow7\nKtdQXlh3WVu+PIx8161buA032yFVn34attkGVlkF3n8/uwTiberFT4Jjjw23bc+bB9tuG3ohAD9+\nzpNIo/TKKyFx9OkDd9wBG20EV1wBX3yR2fpffQXHHQeXXALDhoVnC9ZYI78xu/i1bg0PPwx33gkn\nnwz9+oU6mGvc8p5EJF0o6X1JkyUNlbSypO0lvSVpoqRxknauYd1KSZOqlst3rI3FwIFwzjnQqxe8\n9hqMGRM64+vcufarEzN45BHYbjvo0AEmTYK9925YLOXl5Q3bgCu4gw8OV55mcPHF5Xz0UdwRuTjl\ntSYiqS0wFuhoZoslPQq8APQCbjWzUZJ+C1xqZvumWf9zYCczm1/HfrwmkqEZM2CHHWD69PBsQKof\nfwwJZNAg+Pjjn882N944rHfmmTBrViiw7pw27bvG5r77wlXs4MFw6KFxR+PqI0k1kaZAS0nNgBbA\nLGA50CqaXxa9l47wJrec+vvfwwNl1RMIhO4uTjjh11cn++4baii77w7jx+c2gXiberJttlkFzzwD\nZ50F110X6m2uccn73VmSzgNuBL4HRplZb0kdgZcISULA7mY2M826nwMLgGXAIDO7t4Z9+JVIBn76\nCdq3h4qKzHvV/fFHeO452Hpr2HLL3MdUUVHhTVoJVnX85swJHTmusw489JDXyJIgV1ciee2AUVIZ\ncDjQHlgIPC7pBKAzcL6ZPSWpB3A/cECaTexhZnMkrQO8LGmqmY1Nt6++ffvSoUMHAMrKyujUqdOK\nL6eqs93GPj17djnbbANz51Ywd25m6zdvDq1bV/DVV7DllrmPr7y8vGg+H5+u/3Tq8Rszppzzz4dt\ntqnghhvgpJPij8+nf3m1X1FRQWVlJbmU75pID6CbmfWLpnsDXYBeZrZmynILzaxVDZupWmYA8K2Z\n3ZZmnl+JZGCPPeDii8N4287ly733wpVXep2k2CWlJjID6CKpuSQBXYEPgdmS9gGQ1BX4uPqKklpI\nWi163RI4EHg/z/GWrPfeC8XxYvujTj1LcsmT7vj16xeeIfI6SeOQ1+YsMxsnaSQwEVgS/TsIeA+4\nXVJT4EfgdABJbYB7zewQYD3gSUkWxTnUzEblM95SNnBg6FW3mY8g4wpgt93CTRg9esCECV4nKWXe\n7UkjsGBBuE136lRYf/24o3GNyeLFoWfg116DESNg++3jjshVSUpzlisCQ4ZA9+6eQFzhrbxy6FPt\n0kvhwAPDwFezarqh3yWSJ5ESt3z5z0+oFyOviSRbpsfv5JPDA6zrrRd6PLjqKli0KL+xucLwJFLi\nXn01PES4xx5xR+Iau1at4E9/gokTYebMMO7MwIHZdwDqioPXRErckUeGpiwfqtYVm4kTQzPXjBlw\n001wxBGhx2BXGD6eSApPIunV1k+Wc8XALAzFe+ml4Urlllt84KtC8cK6q9OgQTX3k1UsvCaSbA09\nflK4Up44MRTde/SAY46BHD9U7fLIk0iJ+umn0MPq2WfHHYlzdWva9Ofi+7bbhhrehx/GHZXLhDdn\nlahhw+D++8PY584lzSOPwGWXhWGXt9oq7mhKUyI6YHTxGTgwjH3uXBKdeGL494ADPJEUO2/OKkGT\nJoVierH1k5WO10SSLZ/H78QT4eabQyLxpq3i5VciJehvf/N+slxp8CuS4uc1kRJT1U/WtGnh6WDn\nSoHXSHLPayIurSFD4Le/9QTiSotfkRQvr4mUkKp+spJ0W6/XRJKtkMfPayTFya9ESoj3k+VKnV+R\nFB+viZSQI48MTVmnnx53JM7ll9dIGs77zkrhSST0itqpk/eT5RoPTyQN431nuV+47z7o1St5CcRr\nIskW5/E78cTQ+2/37vD117GF0eh5TaQELFkSksgoH4HeNTK9e8MHH4QTqBdfDH1wucLK+5WIpAsl\nvS9psqShklaWtL2ktyRNlDRO0s41rNtd0jRJH0u6LN+xJtWzz8Kmm8LWW8cdSf2Vl5fHHYJrgGI4\nfjfcAEuXwrXXxh1J41RjTUTSRRms/z8z+3uNG5faAmOBjma2WNKjwAtAL+BWMxsl6bfApWa2b7V1\nmwAfA12B2cB44Hgzm5ZmP426JnLAAaEH1F694o7EuXjMnQs77wz33htuLnF1K0RN5BJgNWD1Wn4y\n6eKvKdBSUjOgBTALWA60iuaXRe9V1xn4xMymm9kSYARweAb7a1Q++ST0lXX00XFHkh2viSRbsRy/\n9deH4cPDydT06XFH07jUVhN52Myuq21lSS1rm29msyXdCswAvgdGmdloSV8CL0XzBOyeZvV2wMyU\n6S8JicWl+Pvfwx/OKqvEHYlz8dprL7j44jCo1euv+99EoeT1Fl9JZcA/gGOAhcDj0XRnYIyZPSWp\nB3CGmR1Qbd2jgW5mdno0fSLQ2czOS7Mf69OnDx06dACgrKyMTp06rWivrTpbKrXpLl3K2XBDuP32\nCtq2jT8en/bpuKfNYO+9K1h7bXjqqfjjKabpqteV0bCRQ4YMKexzIpK6AH8AmgO3m9mTGazTg5AI\n+kXTvYEuQC8zWzNluYVm1qraul2AP5hZ92j6csDM7OY0+2mUNZGHH4ahQ8NdKc65YOHCUB+59lqv\nE9Ym7zURSetXe+si4EjgIKDWZq4UM4AukppLEqFI/iEwW9I+0X66Egro1Y0HNpPUXtLKwPHAMxnu\nt1G45x4466y4o2iY1LMklzzFePxatYKRI+H888Ptvy6/aquJ3CNpAvBnM/sRWAD0IBTFF2WycTMb\nJ2kkMBFYEv07CHgPuF1SU+BHoKrJqg1wr5kdYmbLJPUHRhGS3WAzm5rNL1mKJk+GGTPg4IPjjsS5\n4rP99nDLLeGGk/HjYfXV446odNXanCXpUOB84CFgJOHW3BbAcDMrmmdEG2Nz1tlnhztSrrkm7kic\nK179+sGiRTBiBKjBDTelpWB9Z0VXC2cDhwA3mtm/GrrTXGtsSeTbb6F9e5gyBdq1izsa54rXjz/C\n7rtD375w3q9uyWncClETOUzSGOBF4H3gOOBwSSMkbdrQHbvsDRsG++5bGgmkGNvUXeaK/fg1bx7q\nIzfcAG+9FXc0pam2hw1vAH4LHAvcbGYLzOx3wNXAjYUIzv2aGdx9N5x5ZtyROJcMm2wCgwfDccd5\nR435UFu3J68DdxNqIEeY2SGFDKw+GlNz1ttvh95LP/4YmngfzM5l7Pe/h3fe8Y4aqxSi25MjgbUJ\nd3D53dZFouoqxBOIc/Vz/fXhSv6KK+KOpLT4oFQJ8s034dL800+hdeu4o8mNioqKFU/WuuRJ2vGb\nNw922SWM1X7ssXFHE69CFNYnZBBEncu43HnwQTj00NJJIM4VWuvW8OST0L9/eNbKNVxtNZEfgE9q\nWxdoZWYb5SOw+mgMVyJm8JvfwAMPwB57xB2Nc8k2fDhcdVV4EHGtteKOJh65uhKp7Yn1jhmsv6yh\nAbjMvPpquF1x93T9HTvn6qVnT3j33fDvCy94ob0hamzOisbxqOvny0IG25jdc08oqJfaU7fF/pyB\nq12Sj99NN8GyZV5obyi/xycB5syB0aPDrb3Oudxo1ix0h/LYY+HHZcfvzkqAG26AmTPDAFTOudx6\n7z048MBworbddnFHUziFeE6kakfnSlqzruVcfixbBoMG+RPqzuVLp05w++1w5JHhNnpXP5k0Z60H\njJf0mKTu0bggrkBeeAHatoUddog7kvxIcpu6K53j17NnSCI9e4YTN5e5OpOImV0FbA4MBvoCn0j6\no3fCWBh33538gaecSwIvtGenPsPjbg+cDHQHxhCGuX3ZzC7NX3iZSVJN5K23QoF86dLMlv/pJ/ji\nC1h11fzG5ZxrXE+0F3I8kfOBk4B5wH3AU2a2RFIT4BMzi/2KJElJpFcv2HbbzMd+LisLw3065wqj\nsRTaC5lErgXuN7PpaeZtWQxD1iYliSxaBBttBJ99BmuvHXc0xSFpfS+5XyrV41f1RPt775Xu0LoF\nuzsL+Cew4p4FSWtI2hWgGBJIkjz+OOy3nycQ54pdz56w556h519Xu0yuRCYCO1ad6kfNWO+Y2Y4Z\n7UC6EDgVWA5MAU4BhgBbRIusCcxPtz1JlcDCaN0lZta5hn0k4kpk773hoovgiCPijsQ5V5e5c2Gb\nbeCNN0K/daWmEH1nrdhX6je0mS2XlMl6SGoLnAt0NLPFkh4FjjOz41OW+T9gQQ2bWA6Um9n8TPZX\nzD7/HKZNg4MOijsS51wm1l8/DGR1wQXhVnt/uCG9TJqzPpd0nqSVop/zgc/rsY+mQMso8bQAZleb\nfywwvIZ1lWGMRe+hh+D442HlleOOpLiUynMGjVWpH79zz4XKSnjuubgjKV6ZfEGfCewOzAK+BHYF\nTs9k42Y2G7gVmBGtv8DMRlfNl7QXMNfMPqtpE8DLksZL6pfJPovR8uUhifTpE3ckzrn6WHnl8DT7\nBRfAjz/GHU1xqrNZysz+Axxf13LpSCoDDgfaE2obIyX1MrNh0SI9qfkqBGAPM5sjaR1CMplqZmPT\nLdi3b186dOgAQFlZGZ06dVpx10jV2VJc03fdVYEZ7LhjccRTTNPl5eVFFY9P+/GrPr3yyhW0aQO3\n3VbOFVfEH0+201WvKysryaVMCuvNCYXxrYHmVe+b2Sl1blzqAXQzs37RdG9gVzPrL6kp4epkx+iK\npa5tDQC+NbPb0szLeWF92rRwF9U66zR8W6edFgpzl1zS8G055wrv88+hc2eYOBE23DDuaHKjkLf4\nPgysD3QDXgM2AL7NcPszgC6Smkd9bnUFqm4LPgCYWlMCkdRC0mrR65bAgcD7Ge63QczgmGNy0+nh\n99/DE0/ACSc0fFulKPUsySVPYzl+m2wCZ5/tJ4LpZJJENjOzq4H/mdkQ4GBCXaROZjYOGAlMBCYR\nCuWDotnHUa0pS1IbSVUlrPWAsdEtxm8Dz5rZqEz221BjxoRuScaNg7ffbti2nnoqnMG0bZub2Jxz\n8bj88tBt0WuvxR1JccmkOWucmXWW9C/gbGAuMM7MNilEgJnIdXPWYYfBwQfDSivBgw+G/zTZ3t7X\nrRv07RseXnLOJdvjj4cHECdMCINaJVkhm7MGReOJXAU8A3wI3NzQHRerzz4LZxu9e4e7qb75Bp5/\nPrttzZoF48f7w4XOlYoePaB1ax8gLlWtSSR6On2Rmc03s3+Z2SZmtq6ZlexHeOedcOqp0KIFNG0K\nf/pTuIzNZoyBoUPh6KO9B97aNJY29VLV2I6fFG75vfba0OOvqyOJmNlyIPau3gtl0aLwPMc55/z8\n3iGHwJprhvfrwwyGDIGTTsptjM65eG27bXhw+Kqr4o6kOGRSE7mJ0A38o8D/qt43s6IZSDJXNZHb\nb4c334RKSwaBAAAWVklEQVRHH/3l+2+9FcYW+PjjzK8q3nkHjjsOPv3Uu0twrtTMnw9bbhm6Q9kx\no14Ei08hu4L/Is3bVmqF9WXLwrMcDz0Eu+/+6/lHHQW77Zb5LX7nnhvaTgcMaFBYzrkide+94cab\nsWOTeaJYsMK6mW2c5qdoEkiuvPBCaLbabbf08//4R/jzn0OhvS6LF8OIEd6UlYnG1qZeahrz8Tvl\nlDDy6NChcUcSrzpvUpOU9qvQzOpZJShuf/1r6B+npjOKjh3D1chNN4VkUpvnn4ettoKNN859nM65\n4tC0abgRp0cPOPzw0h28qi6ZNGfdmTLZnPDU+QQz65HPwOqjoc1ZU6aE5zkqK2vvZXf27FBUe++9\n2rs+OPJIOPTQcKbinCttffrAeuvVfXJZbApWE0mz4zJghJl1b+jOc6WhSaRfvzBs7dVX173slVeG\nZPLAA+nnz5sHm20GM2bAGmtkHZJzLiHmzAknl0kbvKqQDxtW9z+gZBpq5s2DkSPhjDMyW/7SS0P9\nZMqU9POHDw9Pu3sCyUxjblMvBX78oE2b8CzZBReEW/sbmzqTiKRnJT0T/TwHfAQ8mf/QCmPQoND8\ntO66mS3fqlUY7ez3v08/f8gQHzfEucbmvPPgiy8a5+BVmdRE9kmZXApMN7Mv8xpVPWXbnLVkSSh+\nP/88bL995uv99FMotD/4IOyT8ul88EGorUyfHopuzrnG46WXwoPK778PzZvXvXzcCtmcNQP4t5m9\nZmZvAP+V1KGhOy4GI0eG+kV9EgjAKqvADTfAZZf98vJ1yBA48URPIM41Rt26wTbbwG2/GvGotGWS\nRB4HlqdML4veS7yqYS+z0bNnGC7ziSfC9LJl8Mgj/mxIfXmberL58ful226DW2+FmTPjjqRwMkki\nzcxscdVE9LqWG2GT4e234T//CbfiZqNJE7j5ZrjiitAsNno0tGsXng9xzjVOVYNXXdpoehzMrCby\nMnCnmT0TTR8OnGdmXQsQX0ayqYn07Am77AIXXZT9fs3ggAPCKIivvRa6S+nfP/vtOeeS7/vvQ830\n4Yd/WTMtNoXsO2tTYChQNTbfl8BJZvZpQ3eeK/VNIrNmhfu6v/gi3G3VEO+8E65mfvghjEWy9toN\n255zLvkeewxuvBHefbd4B68qZN9Zn5lZF2ArYCsz272YEkg2Bg4MY543NIEA7LxzONvYbz9PINnw\nNvVk8+OX3jHHwFprNY7BqzLpO+uPwJ/NbEE0vSbwOzNLZG/6P/wQng15443cbfP++0Oni845B6EP\nvjvugK5dw5AQrVvHHVH+ZNKcNdHMdqj23gQzy6gXfUkXAqcS7vCaApwCDAG2iBZZE5ifbnuSugN/\nJVwxDTaztMPy1qc567774KmnGudDQc65wjrvvHCCec89cUfya4V8TqSppFVSdrwqsEoty68gqS1w\nLrCjmW1HuPI5zsyON7Mdo8TxD+CJNOs2Ae4CugFbAz0ldcxkvzUxC7f1nn9+Q7binHOZufbacNI6\nYULckeRPJklkKPCKpFMlnQq8DNSnG/imQEtJzYAWwOxq848FhqdZrzPwiZlNN7MlwAjg8Hrs91de\nfRWWL4f992/IVlwueZt6svnxq92aa8L114dB6kq1X61MCus3AzcAW0Y/19fUrJRm3dnArYSn3mcB\nC8xsdNV8SXsBc83sszSrtwNSH9n5MnovK2Zwyy3h8jKJo5A555Kp1AevyujmMzN7EXgRQNKekv5m\nZufUtV7UbfzhQHtgITBSUi8zGxYt0pP0VyH11rdvXzp06ABAWVkZnTp1ory8HAhnS6NHw5dfltO3\n789nT6nzfTqe6fLy8qKKx6f9+OV6+vXXKzj5ZLjssnIOPxzefTeeeKpeV1ZWkksZjSciaQfCF/6x\nwBfAE2Z2Z+1rgaQeQDcz6xdN9wZ2NbP+kpoSrk52jK5Yqq/bBfhD1bglki4njO3+q6ugugrrc+eG\n/rGefz7ckuucc4XWpw+sv37o6aIY5L2wLmkLSQMkTQPuJDQtycz2zSSBRGYAXSQ1lyTCqIhTo3kH\nAFPTJZDIeGAzSe0lrQwcDzyT4X5XMIMzz4TTTvMEUoxSz5Jc8vjxy9xNN8HgwfDxx3FHklu11USm\nAfsBh5jZnlHiWFafjZvZOGAkMBGYBAgYFM0+jmpNWZLaRGOWYGbLgP7AKOADwmiKU6mn4cPDk+TX\nXFPfNZ1zLndKdfCqGpuzJB1BOPvfg1APGQHcZ2ZFN6phTc1Z3ozlnCsmixfDdtuFnn4PPjjeWArZ\nd1ZLQnG8J+HK5CHgSTMb1dCd50q6JGIWRizceuvQh41zzhWDp5+GAQPCsyNNshmgPEcK2XfW/8xs\nmJkdCmxAaJq6rKE7zjdvxkoGb1NPNj9+9XfYYbDSSj+PRZR09cqDZjbfzAYVUzfw6cydCxdeCA88\nEEYhdM65YiHBddeFE9xl9aoyF6eMbvEtdqnNWd6M5Zwrdmaw555hAKsTTognhoLVRJIgNYkMGwZ/\n+lMY58OvQpxzxerVV8PjBx9+GM+YI4XsgDExvBkrebxNPdn8+GVvv/3CkNoPPxx3JA1TMknEHyp0\nziXN9deH+kiSxyMqmeasoUPNm7Gcc4nTvTsccUQ4CS4kr4mkkGTrrmv+UKFzLnHGjYOjj4ZPPoHm\nzQu3X6+JVOPNWMnkberJ5sev4Tp3hh12SO547CWTRPyhQudcUl13Xeig8fvv446k/kqmOasUfg/n\nXON1zDHhquSSSwqzP6+JpPAk4pxLug8+CLf9fvoprL56/vfnNRFXErxNPdn8+OXO1lvDAQfA7bfH\nHUn9eBJxzrkiMWBASCILFsQdSea8Ocs554rIKafABhuEYns+eU0khScR51yp+OKL8LjCRx9B69b5\n24/XRFxJ8Db1ZPPjl3sbbwzHHgu33BJ3JJnxJOKcc0Xmyivhvvvgq6/ijqRueW/OknQhcCqwHJgC\nnGxmiyWdC5wNLAWeN7PL06xbCSyM1l1iZp1r2Ic3ZznnSsr554fhc//yl/xsPxE1EUltgbFAxyhx\nPAo8D8wArgAOMrOlklqb2bw0638O7GRm8+vYjycR51xJmTs33PY7aVIotOdakmoiTYGWkpoBLYDZ\nwFnATWa2FCBdAomoQDG6mHiberL58cuf9deHk0+GO+6IO5La5fUL2sxmA7cSrjxmAQvMbDSwBbC3\npLcljZFUU9eJBrwsabykfvmM1Tnnis0xx8A//xl3FLXL66CMksqAw4H2hNrG45JOiPa7ppl1kbQL\n8BiwSZpN7GFmcyStQ0gmU81sbLp99e3blw4dOgBQVlZGp06dKC8vB34+W/Lp4psuLy8vqnh82o9f\nMU3vvDN88UUFTzwBRx3VsO1Vva6srCSX8l0T6QF0M7N+0XRvoAuwMXCzmb0Wvf8psKuZ/beWbQ0A\nvjWz29LM85qIc64kHXVUGG/khBNyu92k1ERmAF0kNZckoCvwIfAUsB+ApC2AlaonEEktJK0WvW4J\nHAi8n+d4XYGlniW55PHjl3/77w8vvxx3FDXLd01kHDASmAhMIhTKBwEPAJtImgIMA04CkNRG0nPR\n6usBYyVNBN4GnjWzUfmM1znnis3++8Po0VCsjS3e7YlzzhUxM+jQAV56CTp2zN12k9Kc5ZxzrgGk\nn69GipEnERcrb1NPNj9+hVHMdRFPIs45V+S6doXXXoOlS+OO5Ne8JuKccwnQqRPcfTfstltutuc1\nEeeca0SKtS7iScTFytvUk82PX+EUa13Ek4hzziXAXnvBhAnw3XdxR/JLXhNxzrmE2HdfuOQSOOig\nhm/LayLOOdfIFGNdxJOIi5W3qSebH7/CKsa6iCcR55xLiJ12gi+/DKMeFguviTjnXILkqmt4r4k4\n51wjVGxNWp5EXKy8TT3Z/PgVXrF1De9JxDnnEmTzzaFpU/joo7gjCbwm4pxzCXPqqbDDDtC/f/bb\n8JqIc841UsVUF/Ek4mLlberJ5scvHsXUNXzek4ikCyW9L2mypKGSVo7eP1fSVElTJN1Uw7rdJU2T\n9LGky/Idq3POJcG664Yhc8ePjzuSPNdEJLUFxgIdzWyxpEeB54EZwBXAQWa2VFJrM5tXbd0mwMdA\nV2A2MB443sympdmP10Scc43KxRdDq1Zw9dXZrZ+kmkhToKWkZkALQkI4C7jJzJYCVE8gkc7AJ2Y2\n3cyWACOAwwsQr3POFb1iqYvkNYmY2WzgVsKVxyxggZmNBrYA9pb0tqQxknZOs3o7YGbK9JfRe66E\neJt6svnxi0+xdA2f1yQiqYxw9dAeaEu4IjkBaAasaWZdgEuBx/IZh3POlZqWLWGXXeBf/4o3jmZ5\n3v7+wOdm9g2ApCeB3QlXGE8AmNl4ScslrW1m/01ZdxawUcr0BtF7afXt25cOHToAUFZWRqdOnSgv\nLwd+Plvy6eKbLi8vL6p4fNqPX5Km998f7r+/ghYt6l6+6nVlZSW5lO/CemdgMLAL8BPwAKFAvgRo\nZ2YDJG0BvGxm7aut2xT4iFBYnwOMA3qa2dQ0+/HCunOu0fn3v+G002DKlPqvm4jCupmNA0YCE4FJ\ngIBBhGSyiaQpwDDgJABJbSQ9F627DOgPjAI+AEakSyAu2VLPklzy+PGLVzF0DZ/v5izM7Frg2jSz\neqdZdg5wSMr0i8Bv8hedc84lV7NmYcjcV15peNfw2fK+s5xzLsEGDoRx4+DBB+u3XiKas5xzzuVX\n3F3DexJxsfI29WTz4xe/zTeHJk3i6xrek4hzziWYBAccEK5GYtl/KdQSvCbinGvMhg+HESPg6acz\nXydXNRFPIs45l3BffQW/+Q3Mmxfu2MqEF9ZdSfA29WTz41cc1lsP2rePp2t4TyLOOVcC4qqLeHOW\nc86VgBdfhIcfhqFDM1veayIpPIk45xo7s3CnVqa8JuJKgrepJ5sfv+JRnwSSS55EnHPOZc2bs5xz\nrhHy5iznnHOx8yTiYuVt6snmx895EnHOOZc1r4k451wj5DUR55xzsfMk4mLlberJ5sfP5T2JSLpQ\n0vuSJksaKmkVSQMkfSlpQvTTvYZ1KyVNkjRR0rh8x+oK77333os7BNcAfvxchp0GZ0dSW+BcoKOZ\nLZb0KHB8NPs2M7utjk0sB8rNbH4+43TxWbBgQdwhuAbw4+cK0ZzVFGgpqRnQApgVvZ9JQUckpMkt\n15f12W6vPuvVtWy28+v7fjHIZWzFcOzqWiabecV6/Erxb6+uZeo7L5/HLq9f0GY2G7gVmEFIHgvM\nrKqz4v6S3pN0n6RWNW0CeFnSeEn98hlrQ5Xif+RCJJHKyspa91EonkTqnlesx68U//bqWqaYkkhe\nb/GVVAb8AzgGWAiMBB4HXgbmmZlJugFoY2anplm/jZnNkbROtE5/MxubZjm/v9c55+opF7f45rUm\nAuwPfG5m3wBIegLY3cyGpSxzL/BsupXNbE7079eSngQ6A79KIrn4IJxzztVfvusNM4AukppLEtAV\nmCpp/ZRljgLer76ipBaSVotetwQOTLecc865+OT1SsTMxkkaCUwElgATgEHAYEmdCHdfVQJnQGi+\nAu41s0OA9YAno6aqZsBQMxuVz3idc87VT0l0e+Kccy4eibh91jnnXHHyJOKccy5rJZ1EouL8eEkH\nxR2Ly5ykjpLulvSYpDPjjsfVj6TDJQ2SNFzSAXHH4+pH0sbR83uPZbR8KddEJF0LfAt8aGYvxB2P\nq5/ojr4hZnZS3LG4+oueE7vFzIr6QWGXnqTHzOzYupYr+isRSYMlfSVpcrX3u0uaJuljSZelWW9/\n4EPgazLrYsXlWLbHLlrmUOA5wJN/TBpy/CJXAX/Lb5SuJjk4fhkp+iQCPAB0S31DUhPgruj9rYGe\nkjpG83pL+gvQE9gV6AWcVtCIXZVsjt1tUU8Fz5rZwcCJhQ7arZDt8Wsr6SbgBTPzbn7jk/XfX9Xi\nmewk30+sN5iZjZXUvtrbnYFPzGw6gKQRwOHANDN7GHi4akFJJwHzChWv+1m2x07SPpIuB1YBni9o\n0G6FBhy/cwkPFq8haTMzG1TQwB3QoOO3lqS7gU6SLjOzm2vbT9EnkRq0A2amTH9J+HB+xcweKkhE\nLlN1Hjszew14rZBBuYxlcvzuBO4sZFAuY5kcv2+AszLdYBKas5xzzhWppCaRWcBGKdMb8PM4Ja64\n+bFLNj9+yZbz45eUJCJ+WeQZD2wmqb2klQmjJT4TS2SuLn7sks2PX7Ll/fgVfRKRNAx4E9hC0gxJ\nJ5vZMsKwu6OAD4ARZjY1zjjdr/mxSzY/fslWqONX0g8bOuecy6+ivxJxzjlXvDyJOOecy5onEeec\nc1nzJOKccy5rnkScc85lzZOIc865rHkScc45lzVPIq5oSfq2wPsbVNUtdgH3eb6k5vVcp0LSVEmH\nRNNjJO1Yj/W3l/TbDJbbU9IH1cejcC6VJxFXzHL6JKykprXuzOx0M5uWy31G+61tXIYLgBb13KQB\nvczsuSxD6gTUOWS0mY3NZDnXuHkScYkiqbWkkZL+Hf3sFr2/i6Q3Jb0raaykzaP3+0h6WtIrwOho\nrJIxkh6PzuZTx55ZcUYv6VtJN0h6L9ruOtH7m0h6S9IkSdenu1qK+iWaJmmIpCnABpIGShonaYqk\nAdFy5wJtgTFRfEg6MNrfO5IelVRTgqmemE6SNFHSZEm7RNtqoTC63dvR53KopJWA64BjJU2QdExN\nn51zGTEz//GfovwBFqV5byiwe/R6Q+DD6PVqQJPodVdgZPS6DzADaBVN7wPMB9oQvojfTNneGGDH\n6PVy4KDo9c3AFdHrZ4Fjo9dn1BBje2ApsEvKe2XRv02i/WwTTX8OrBm9Xpswjsqq0fSlwNVptr8i\nzpTpv0ev9wKmRK9vJFyxALQCPgJWjT6TO1LWT/vZpfwuk+P+v+A/xfuT1EGpXOO1P7BlShPRatHZ\nehnwUHQWbfxywLWXzWxhyvQ4M5sDIOk9oAMhmaT6ycyqxnd/N9ovwG6EkeAAhgG31BDndDMbnzJ9\nvKR+UVzrA1sB7/PLXla7RO+/Ef1+KwFv1bD96oYDmNnrklaXtAZwIHCopEuiZVbml92AV6nts3Ou\nVv6fxSWNgF3NbMkv3pT+BrxqZkdFQ4KOSZn9v2rb+Cnl9TLS/x0sqWGZ1DpNbbWOFfuU1AH4HbCT\nmS2S9ACQrpguYJSZnVDLdmtSvX5k0faONrNPfrETqUu1Za+n5s/OuVp5TcQVs3Rf0qOA81csIG0f\nvVyDnwfXOTlP+wZ4G+gRvT4+w/XXAL4DvpW0HpB6Z9SiaH7VtveQtCmsqGlkWp84LlpnT2ChmX0L\nvASctyIgqVP08tuUfVbFl8vPzjUinkRcMVs1GgdhZvTvBYQvxZ2jwvb7hLoEhGalmyS9S/3+X1sG\nr1NdCFwUNYNtCiysYbkV65vZZOA9YCrwCDA2Zbl7gRclvWJm8whf4sMlTSI0sf0mw9/hR0kTgIHA\nKdH71wMrRcX2KYSCOoQrja2qCuvAn8nus3POxxNxrj4krWpmP0SvjwOON7MjCxzDGOB3ZjahAPvq\nADxjZtvle18umfysw7n62Sm67XcScBah1lFo3wAPVj1smC9R09gzwNf53I9LNr8Scc45lzW/EnHO\nOZc1TyLOOeey5knEOedc1jyJOOecy5onEeecc1n7f3M2/kX5aWFaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f6ee290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.semilogx(beta_values, results)\n",
    "plt.xlabel('Learning rate [beta]')\n",
    "plt.ylabel('Accuracy [%]')\n",
    "plt.grid(True)\n",
    "plt.title('Regularization Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second, L2 regularization for *neural network* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes_size = 1024   # This is the number of the nodes in the hidden layer\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    '''\n",
    "    Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch.  \n",
    "    '''\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)  # Learning rate for regularization\n",
    "    \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes_size]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes_size]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_nodes_size, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1), weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + \n",
    "                          beta * tf.nn.l2_loss(weights1) + \n",
    "                          beta * tf.nn.l2_loss(weights2))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden, weights2) + biases2)\n",
    "    \n",
    "    test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3488.848389\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 30.3%\n",
      "Minibatch loss at step 500: 21.181538\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 1000: 0.945614\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 1500: 0.584087\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 2000: 0.607787\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 2500: 0.719979\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 3000: 0.763063\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.4%\n",
      "Test accuracy: 90.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps+1):\n",
    "        '''\n",
    "        Pick an offset within the training data, which has been randomized.\n",
    "        Note: we could use better randomization across epochs.\n",
    "        '''\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        '''\n",
    "        Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        and the value is the numpy array to feed to it.\n",
    "        '''\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta: 0.01}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from assignment 2 that the final result without regularization is:\n",
    "\n",
    "###### Minibatch loss at step 3000: *1.388752*\n",
    "###### Minibatch accuracy: *82.0%*\n",
    "###### Validation accuracy: *81.7%*\n",
    "###### Test accuracy: *88.8%*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different values for beta is very slow for neural network.\n",
    "TODO: try to make it faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    '''\n",
    "    Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch.  \n",
    "    '''\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)  # Learning rate for regularization\n",
    "    \n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + \n",
    "                          beta * tf.nn.l2_loss(weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 47.551231\n",
      "Minibatch accuracy: 9.0%\n",
      "Validation accuracy: 13.2%\n",
      "Test accuracy: 16.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps+1):\n",
    "        '''\n",
    "        Pick an offset within the training data, which has been randomized.\n",
    "        Note: we could use better randomization across epochs.\n",
    "        '''\n",
    "        #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        '''\n",
    "        Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        and the value is the numpy array to feed to it.\n",
    "        '''\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta: 0.01}  # Use 0.01 for beta\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
