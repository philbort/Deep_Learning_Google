{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, L2 regularization for *logistic regression* model.\n",
    "\n",
    "The multinomial logistic regression model with stochastic gradient descent is adapted from assignment 2:2_fullyconnected.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    '''\n",
    "    Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch.  \n",
    "    '''\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)  # Learning rate for regularization\n",
    "    \n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + \n",
    "                          beta * tf.nn.l2_loss(weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 46.824379\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 12.8%\n",
      "Minibatch loss at step 500: 0.733136\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 1000: 0.800147\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 1500: 0.566087\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 2000: 0.648335\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2500: 0.782492\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 3000: 0.784389\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.4%\n",
      "Test accuracy: 88.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps+1):\n",
    "        '''\n",
    "        Pick an offset within the training data, which has been randomized.\n",
    "        Note: we could use better randomization across epochs.\n",
    "        '''\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        '''\n",
    "        Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        and the value is the numpy array to feed to it.\n",
    "        '''\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta: 0.01}  # Use 0.01 for beta\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from assignment 2 that the final result without regularization is:\n",
    "\n",
    "###### Minibatch loss at step 3000: *1.061334*\n",
    "###### Minibatch accuracy: *77.3%*\n",
    "###### Validation accuracy: *79.0%*\n",
    "###### Test accuracy: *86.4%*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try different values for beta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% finished\n",
      "16.0% finished\n",
      "33.0% finished\n",
      "50.0% finished\n",
      "66.0% finished\n",
      "83.0% finished\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "beta_values = [pow(10, i) for i in np.arange(-4, -1, 0.1)] # from 0.0001 to 0.1\n",
    "results = np.zeros(len(beta_values))\n",
    "\n",
    "for i, beta_value in enumerate(beta_values):\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        \n",
    "        tf.initialize_all_variables().run()        \n",
    "        for step in range(num_steps+1):\n",
    "            \n",
    "            '''\n",
    "            Pick an offset within the training data, which has been randomized.\n",
    "            Note: we could use better randomization across epoch\n",
    "            '''\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            '''\n",
    "            Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            and the value is the numpy array to feed to it.\n",
    "            '''\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : beta_value}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            \n",
    "        results[i] = accuracy(test_prediction.eval(), test_labels)\n",
    "        # Print the progress\n",
    "        if(i%5 == 0):\n",
    "            print('%0.1f%% finished' %(100*i/len(beta_values)))\n",
    "            \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEdCAYAAADNU1r0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nplR0rQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14bbb07d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.semilogx(beta_values, results)\n",
    "plt.xlabel('Learning rate [beta]')\n",
    "plt.ylabel('Accuracy [%]')\n",
    "plt.grid(True)\n",
    "plt.title('Regularization Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, L2 regularization for *neural network* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes_size = 1024   # This is the number of the nodes in the hidden layer\n",
    "betha = 0.01  # Learning rate for regularization\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    '''\n",
    "    Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch.  \n",
    "    '''\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes_size]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes_size]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_nodes_size, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1), weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + \n",
    "                          betha * tf.nn.l2_loss(weights1) + \n",
    "                          betha * tf.nn.l2_loss(weights2))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden, weights2) + biases2)\n",
    "    \n",
    "    test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3472.870850\n",
      "Minibatch accuracy: 8.6%\n",
      "Validation accuracy: 29.2%\n",
      "Minibatch loss at step 500: 21.234951\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 1000: 0.948464\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 1500: 0.581729\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 2000: 0.606362\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 2500: 0.714091\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 3000: 0.763866\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 83.5%\n",
      "Test accuracy: 90.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps+1):\n",
    "        '''\n",
    "        Pick an offset within the training data, which has been randomized.\n",
    "        Note: we could use better randomization across epochs.\n",
    "        '''\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        '''\n",
    "        Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        and the value is the numpy array to feed to it.\n",
    "        '''\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from assignment 2 that the final result without regularization is:\n",
    "\n",
    "###### Minibatch loss at step 3000: *1.388752*\n",
    "###### Minibatch accuracy: *82.0%*\n",
    "###### Validation accuracy: *81.7%*\n",
    "###### Test accuracy: *88.8%*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3517.083496\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 26.5%\n",
      "Test accuracy: 81.9%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
