{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First, L2 regularization for *logistic regression* model.\n",
    "\n",
    "The multinomial logistic regression model with stochastic gradient descent is adapted from assignment 2:2_fullyconnected.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    '''\n",
    "    Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch.  \n",
    "    '''\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)  # Learning rate for regularization\n",
    "    \n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + \n",
    "                          beta * tf.nn.l2_loss(weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 48.784027\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 19.9%\n",
      "Minibatch loss at step 500: 0.741133\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 1000: 0.799713\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 1500: 0.566128\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 2000: 0.648393\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 2500: 0.782520\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 3000: 0.784389\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.4%\n",
      "Test accuracy: 88.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps+1):\n",
    "        '''\n",
    "        Pick an offset within the training data, which has been randomized.\n",
    "        Note: we could use better randomization across epochs.\n",
    "        '''\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        '''\n",
    "        Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        and the value is the numpy array to feed to it.\n",
    "        '''\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta: 0.01}  # Use 0.01 for beta\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from assignment 2 that the final result without regularization is:\n",
    "\n",
    "###### Minibatch loss at step 3000: *1.061334*\n",
    "###### Minibatch accuracy: *77.3%*\n",
    "###### Validation accuracy: *79.0%*\n",
    "###### Test accuracy: *86.4%*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try different values for beta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% finished\n",
      "16.0% finished\n",
      "33.0% finished\n",
      "50.0% finished\n",
      "66.0% finished\n",
      "83.0% finished\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "beta_values = [pow(10, i) for i in np.arange(-4, -1, 0.1)] # from 0.0001 to 0.1\n",
    "results = np.zeros(len(beta_values))\n",
    "\n",
    "for i, beta_value in enumerate(beta_values):\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        \n",
    "        tf.initialize_all_variables().run()        \n",
    "        for step in range(num_steps+1):\n",
    "            \n",
    "            '''\n",
    "            Pick an offset within the training data, which has been randomized.\n",
    "            Note: we could use better randomization across epoch\n",
    "            '''\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            '''\n",
    "            Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            and the value is the numpy array to feed to it.\n",
    "            '''\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta : beta_value}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            \n",
    "        results[i] = accuracy(test_prediction.eval(), test_labels)\n",
    "        # Print the progress\n",
    "        if(i%5 == 0):\n",
    "            print('%.1f%% finished' %(100*i/len(beta_values)))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEdCAYAAADNU1r0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f5f6810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.semilogx(beta_values, results)\n",
    "plt.xlabel('Learning rate [beta]')\n",
    "plt.ylabel('Accuracy [%]')\n",
    "plt.grid(True)\n",
    "plt.title('Regularization Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second, L2 regularization for *neural network* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes_size = 1024   # This is the number of the nodes in the hidden layer\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    '''\n",
    "    Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch.  \n",
    "    '''\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)  # Learning rate for regularization\n",
    "    \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes_size]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes_size]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_nodes_size, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1), weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + \n",
    "                          beta * tf.nn.l2_loss(weights1) + \n",
    "                          beta * tf.nn.l2_loss(weights2))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden, weights2) + biases2)\n",
    "    \n",
    "    test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3499.299561\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 27.9%\n",
      "Minibatch loss at step 500: 21.148560\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 1000: 0.944835\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 1500: 0.576774\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 2000: 0.608200\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 2500: 0.715497\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 3000: 0.767823\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.3%\n",
      "Test accuracy: 90.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "beta_value = 0.01\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps+1):\n",
    "        '''\n",
    "        Pick an offset within the training data, which has been randomized.\n",
    "        Note: we could use better randomization across epochs.\n",
    "        '''\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        '''\n",
    "        Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        and the value is the numpy array to feed to it.\n",
    "        '''\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta: beta_value}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from assignment 2 that the final result without regularization is:\n",
    "\n",
    "###### Minibatch loss at step 3000: *1.388752*\n",
    "###### Minibatch accuracy: *82.0%*\n",
    "###### Validation accuracy: *81.7%*\n",
    "###### Test accuracy: *88.8%*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different values for beta is very slow for neural network.\n",
    "TODO: try to make it faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    '''\n",
    "    Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch.  \n",
    "    '''\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)  # Learning rate for regularization\n",
    "    \n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + \n",
    "                          beta * tf.nn.l2_loss(weights))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 19.329138\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 10.2%\n",
      "Minibatch loss at step 500: 0.119919\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 67.8%\n",
      "Minibatch loss at step 1000: 0.029164\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.0%\n",
      "Minibatch loss at step 1500: 0.016403\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 2000: 0.011571\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch loss at step 2500: 0.008998\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.9%\n",
      "Minibatch loss at step 3000: 0.007388\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.0%\n",
      "Test accuracy: 77.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "beta_value = 0.0  # No regularization shows overfitting even more obviously\n",
    "num_batch = 5     # Limit of the number of batches\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps+1):\n",
    "        '''\n",
    "        Pick an offset within the training data, which has been randomized.\n",
    "        Note: we could use better randomization across epochs.\n",
    "        '''\n",
    "        \n",
    "        # \"step % num_batch\" limits the training set to the same batches over and over again\n",
    "        offset = ( (step % num_batch) * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        '''\n",
    "        Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        and the value is the numpy array to feed to it.\n",
    "        '''\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta: beta_value}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Discussion:\n",
    "By limiting the training batch numbers to a small number and getting rid of regularization,\n",
    "overfitting is pretty obvious.\n",
    "The training accuracy goes to 100% but not too well for validation and test datasets.\n",
    "That's because the training algorithm is overfitting the training dataset and not general\n",
    "enough for other datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes_size = 1024   # This is the number of the nodes in the hidden layer\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    '''\n",
    "    Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch.  \n",
    "    '''\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)  # Learning rate for regularization\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes_size]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes_size]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_nodes_size, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    # Training computation.\n",
    "    # \"relu\" before \"dropout\"\n",
    "    layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    drop_out = tf.nn.dropout(layer1, keep_prob)\n",
    "    logits = tf.matmul(drop_out, weights2) + biases2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + \n",
    "                          beta * tf.nn.l2_loss(weights1) + \n",
    "                          beta * tf.nn.l2_loss(weights2))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_hidden = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden, weights2) + biases2)\n",
    "    \n",
    "    test_hidden = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_hidden, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3593.848145\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 23.7%\n",
      "Minibatch loss at step 500: 21.356874\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 1000: 1.014751\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 1500: 0.654728\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.1%\n",
      "Minibatch loss at step 2000: 0.669211\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2500: 0.755356\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 3000: 0.853770\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 83.0%\n",
      "Test accuracy: 90.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "beta_value = 0.01\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps+1):\n",
    "        '''\n",
    "        Pick an offset within the training data, which has been randomized.\n",
    "        Note: we could use better randomization across epochs.\n",
    "        '''\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        '''\n",
    "        Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        and the value is the numpy array to feed to it.\n",
    "        '''\n",
    "        feed_dict = {tf_train_dataset : batch_data, \n",
    "                     tf_train_labels : batch_labels, \n",
    "                     beta: beta_value,\n",
    "                     keep_prob: 0.5}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with 2 layers and decay learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "# Try two layers\n",
    "hidden_nodes_size1 = 1024\n",
    "hidden_nodes_size2 = 100  \n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    '''\n",
    "    Input data. For the training data, we use a placeholder that will be fed\n",
    "    at run time with a training minibatch.  \n",
    "    '''\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)  # Learning rate for regularization\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    # Variables.\n",
    "    # Need to specify a small stddev, otherwise won't converge\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_nodes_size1],\n",
    "                                               stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes_size1]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_nodes_size1, hidden_nodes_size2],\n",
    "                                               stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases2 = tf.Variable(tf.zeros([hidden_nodes_size2]))\n",
    "    weights3 = tf.Variable(tf.truncated_normal([hidden_nodes_size2, num_labels],\n",
    "                                              stddev=np.sqrt(2.0 / (image_size * image_size))))\n",
    "    biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    # drop_out1 = tf.nn.dropout(layer1, keep_prob)     #dropout actually made it worse\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, weights2) + biases2)\n",
    "    #drop_out2 = tf.nn.dropout(layer2, keep_prob)\n",
    "    logits = tf.matmul(layer2, weights3) + biases3\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels) + \n",
    "                          beta * tf.nn.l2_loss(weights1) + \n",
    "                          beta * tf.nn.l2_loss(weights2) + \n",
    "                          beta * tf.nn.l2_loss(weights3))\n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.8, staircase = True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    valid_hidden1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_hidden2 = tf.nn.relu(tf.matmul(valid_hidden1, weights2) + biases2)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_hidden2, weights3) + biases3)\n",
    "    \n",
    "    test_hidden1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_hidden2 = tf.nn.relu(tf.matmul(test_hidden1, weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_hidden2, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 11.216919\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 30.2%\n",
      "Minibatch loss at step 500: 0.699115\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 1000: 0.841523\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 1500: 0.608489\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 2000: 0.625564\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 2500: 0.743373\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 3000: 0.780745\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.2%\n",
      "Test accuracy: 91.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "beta_value = 0.01\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps+1):\n",
    "        '''\n",
    "        Pick an offset within the training data, which has been randomized.\n",
    "        Note: we could use better randomization across epochs.\n",
    "        '''\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        '''\n",
    "        Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        and the value is the numpy array to feed to it.\n",
    "        '''\n",
    "        feed_dict = {tf_train_dataset : batch_data, \n",
    "                     tf_train_labels : batch_labels, \n",
    "                     beta: beta_value,\n",
    "                     keep_prob: 0.75}\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
